{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674ba46b",
   "metadata": {},
   "source": [
    "## Explainable AI: Lime und Saliency Maps\n",
    "\n",
    "In der heutigen Übung möchten wir zwei Modelle trainieren:\n",
    "1) Modell zur Vorhersage von Hauspreisen\n",
    "2) Modell zur Klassifikation von Bildern\n",
    "\n",
    "Wir möchten ExplainableAI Methoden nutzen, um ein Verständnis über die Funktionsweise der Modelle und vor allem eine Erklärung für Nutzende zu ermöglichen. Wir interpretieren einerseits selbst die Ausgaben und nutzen andererseits LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec659376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround to show html rendering in VSCode notebooks\n",
    "import sys\n",
    "from IPython import display\n",
    "sys.modules['IPython.core.display'] = display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca9c05",
   "metadata": {},
   "source": [
    "### Modell zur Vorhersage von Hauspreisen\n",
    "\n",
    "In Übung 2 haben wir bereits mit folgendem Datensatz gearbeitet: https://www.kaggle.com/datasets/yasserh/housing-prices-dataset/data (License: CCO: Public Domain).\n",
    "\n",
    "Lade ihn von Kagglehub herunter, exploriere und lade ihn in ein Pandas Dataframe. Der Dataframe soll der Variablen ```df``` zugewiesen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a09fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "os.environ['KAGGLEHUB_CACHE'] = 'data'\n",
    "# TODO: Datensatz von kagglehub herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18311617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Datensatz in einen Dataframe df laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unser Datensatz hat {df.shape[0]} Samples mit {df.shape[1]-1} Features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc797a",
   "metadata": {},
   "source": [
    "Wir sehen, dass einige Werte als object (bzw. String) vorliegen. Das mag unser Machine Learning Modell nicht, deswegen müssen wir zuerst die String-Werte in Zahlen (int) transformieren. In der Regel nimmt man für no=0 und yes=1. Bei mehreren Kategorien, wie zum Beispiel bei ```df['funishingstatus']``` zählen wir die Zahlen einfach hoch. Für das Ersetzen im Datensatz können wir die Funktion ```df.replace``` nutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9745b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['furnishingstatus'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "df = df.replace({\"no\": 0, \"yes\": 1})\n",
    "df = df.replace({\"unfurnished\": 0, \"semi-furnished\": 1, \"furnished\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7195e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a73966",
   "metadata": {},
   "source": [
    "Wie im Vorlesungsbeispiel splitten wir den Datensatz in:\n",
    "1) x: abhängigen Variablen, d.h. alle Features auf welchen unser Modell trainieren soll\n",
    "2) y: unabhängige Variablen, d.h. das Feature, welches unser Modell vorhersagen soll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into independent and dependent variables\n",
    "# TODO: Siehe Vorlesungsbeispiel\n",
    "\n",
    "# dividing the dataset into training and testing sets\n",
    "# TODO: Siehe Vorlesungsbeispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c16250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a91743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the training data\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243805cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Metriken berechnen\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Metriken ausgeben\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R²-Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e507f8f",
   "metadata": {},
   "source": [
    "### Lime zur Erklärung von Hauspreis-Prognosen\n",
    "\n",
    "Wie in der Vorlesung möchten wir nun einen Hauspreis auf Basis der 12 Features vorhersagen lassen und mit Lime eine Übersicht bekommen welche Features wie auf den Preis eingewirkt haben. \n",
    "\n",
    "Passen Sie den Code aus der Vorlesung so an, dass es für ein Regressions-Modell funktioniert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import numpy as np\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "lime_explainer = # TODO: siehe Vorlesungsbeispiel, aber Achtung Sie müssen einige Parameter anpassen ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f153be",
   "metadata": {},
   "source": [
    "Wir lassen uns für den ersten Datenpunkt im Testdatensatz den Preis berechnen und erklären. Sie können natürlich auch beliebige Werte ausprobieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bdf56",
   "metadata": {},
   "source": [
    "Passen Sie den Code aus der Vorlesung auf dieses Modell an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = lime_explainer.explain_instance(data_row=x_test.iloc[1], predict_fn=model.predict, num_features=12)\n",
    "explanation.show_in_notebook(show_table = True, show_all = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1829d7",
   "metadata": {},
   "source": [
    "Interpretieren Sie erst selbst die Ausgabe von Lime und prüfen Sie, ob ein LLM Ihrer Wahl zur selben Erklärung kommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb05a7",
   "metadata": {},
   "source": [
    "### Modell zur Klassifikation von Bildern\n",
    "\n",
    "Im folgenden nutzen wir ein vortrainiertes vgg16 Modell, welches auf Basis des ImageNet Datensatzes trainiert wurde und 1000 Klassen unterscheiden kann. \n",
    "\n",
    "Based on: https://keisen.github.io/tf-keras-vis-docs/examples/attentions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c30147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3465841",
   "metadata": {},
   "source": [
    "Wir laden uns drei JPG Bilder aus dem Ordner imgs. Nutzen Sie gerne eigene Bilder und probieren verschiedene Klassen durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "image_titles = ['Cat', 'Dog', 'Turtle']\n",
    "\n",
    "# cat and dog images are generated by Mistral Medium 3.1\n",
    "img1 = keras.preprocessing.image.load_img(os.path.join('imgs', '9-cat.jpg'), target_size=(224, 224))\n",
    "img2 = keras.preprocessing.image.load_img(os.path.join('imgs', '9-dog.jpg'), target_size=(224, 224))\n",
    "img3 = keras.preprocessing.image.load_img(os.path.join('imgs', '2-turtle.JPG'), target_size=(224, 224))\n",
    "images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
    "\n",
    "# Rendering\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3cdd1",
   "metadata": {},
   "source": [
    "Um ein gutes Ergebnis zu erreichen müssen wir die Bilder durch einen vgg16 spezifische Vorverarbeitung schicken. Dabei werden sie normalisiert und auf den richtigen Wertebereich angepasst. Anschließend sagt das Modell mit ```model.predict(np.asarray(X))``` für alle Bilder die Klassen vorher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb544ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing input data for VGG16\n",
    "X = keras.applications.vgg16.preprocess_input(images)\n",
    "\n",
    "predictions = model.predict(np.asarray(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f17a09",
   "metadata": {},
   "source": [
    "Unser Modell gibt uns für jedes Bild jeweils 1000 Werte aus. Für jede Klasse aus dem ImageNet Datensatz wird ein Wert ausgegeben, je höher dieser ist, desto eher gehört das Bild zur entsprechenden Klasse. Eine Übersicht über alle Klassen finden Sie hier: https://gist.github.com/maraoz/388eddec39d60c6d52d4\n",
    "\n",
    "Über ```np.argmax``` wählen wir die Klasse mit dem höchsten Prediction Wert aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ce59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [np.argmax(predictions[0]), np.argmax(predictions[1]), np.argmax(predictions[2])]\n",
    "\n",
    "for i in range(len(image_titles)):\n",
    "    print(f\"Prediction {i}. Bild: \", classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0292a82",
   "metadata": {},
   "source": [
    "### Saliency Maps\n",
    "\n",
    "Nun möchten wir verdeutlichen welche Teile des Bilds für das Modell wichtig und welche nicht wichtig zur Klassifikation der drei Bilder sind. Im folgenden nutzen wir die jeweiligen Attentions aus der Lib ```tf-keras-vis```: https://keisen.github.io/tf-keras-vis-docs/examples/attentions.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b310a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "\n",
    "replace2linear = ReplaceToLinear()\n",
    "score = CategoricalScore(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c82556",
   "metadata": {},
   "source": [
    "Im folgenden ist einfach der Code von GradCam hier reinkopiert. Probieren Sie verschiedene Attribution Methoden aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ae369",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model, model_modifier=replace2linear, clone=True)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score, X, penultimate_layer=-1)\n",
    "\n",
    "# Render\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)  # overlay\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357386d",
   "metadata": {},
   "source": [
    "Nutzen Sie ein LLM Ihrer Wahl, um sich die Ausgabe erklären zu lassen. Bei Mistral hat folgender Prompt ganz gut funktioniert:\n",
    "\n",
    "\"Wir haben ein VGG16 Modell zur Bildklassifikation auf Basis der ImageNet Klassen. Das Modell hat die Klassen Egyptian cat vorhergesagt. Die Ausgabe von tf_keras_vis.gradcam ist das Bild im Anhang. Erkläre mir was dies zu bedeuten hat.\"\n",
    "\n",
    "### Ausgabe von Mistral:\n",
    "\n",
    "Das Bild, das du hochgeladen hast, ist eine Grad-CAM (Gradient-weighted Class Activation Mapping)-Visualisierung. Diese Technik wird verwendet, um zu verstehen, welche Bereiche eines Bildes am stärksten zur Klassifizierungsentscheidung eines neuronalen Netzes beigetragen haben. Hier ist eine detaillierte Erklärung, was das bedeutet und wie du die Visualisierung interpretieren kannst:\n",
    "\n",
    "#### Was ist Grad-CAM?\n",
    "\n",
    "Grad-CAM ist eine Methode, um die Aufmerksamkeit eines Convolutional Neural Networks (CNN) wie VGG16 auf bestimmte Bildbereiche zu visualisieren.\n",
    "Es kombiniert die Gradienten (Ableitungen) der Zielklasse (hier: \"Egyptian cat\") mit den Feature-Maps der letzten Convolutional-Schicht des Modells.\n",
    "Das Ergebnis ist eine Heatmap, die zeigt, welche Bildbereiche für die Klassifizierung am wichtigsten waren.\n",
    "\n",
    "#### Interpretation deiner Grad-CAM\n",
    "\n",
    "\n",
    "1) Farben und Intensität:\n",
    "- Rote/Gelbe Bereiche: Diese Bereiche haben den stärksten Einfluss auf die Klassifizierung als \"Egyptian cat\". Das Modell hat sich besonders auf den Kopf und die Augen der Katze konzentriert.\n",
    "- Blaue/Lila Bereiche: Diese Bereiche haben kaum oder keinen Einfluss auf die Klassifizierung.\n",
    "\n",
    "\n",
    "2) Kopf und Augen:\n",
    "- Die Grad-CAM zeigt, dass das Modell die Augen, Ohren und den Kopf der Katze als entscheidend für die Klassifizierung als \"Egyptian cat\" identifiziert hat. Das ist sinnvoll, da diese Merkmale oft charakteristisch für Katzenrassen sind.\n",
    "\n",
    "3) Körper und Hintergrund:\n",
    "\n",
    "- Der Körper der Katze und der Hintergrund sind kaum hervorgehoben, was bedeutet, dass sie für die Klassifizierung weniger relevant waren.\n",
    "\n",
    "\n",
    "#### Warum ist das nützlich?\n",
    "\n",
    "- Modellinterpretation: Grad-CAM hilft dir zu verstehen, welche visuellen Merkmale das Modell gelernt hat, um eine bestimmte Klasse zu erkennen.\n",
    "- Fehleranalyse: Falls das Modell eine falsche Klassifizierung vornimmt, kann Grad-CAM zeigen, welche Bildbereiche das Modell fälschlicherweise als relevant eingestuft hat.\n",
    "- Vertrauen: Es erhöht das Vertrauen in das Modell, wenn die hervorgehobenen Bereiche mit menschlichen Intuitionen übereinstimmen (z. B. Kopf und Augen für eine Katze).\n",
    "\n",
    "#### Technische Details\n",
    "\n",
    "- Letzte Convolutional-Schicht: Grad-CAM nutzt die Feature-Maps der letzten Convolutional-Schicht (bei VGG16 ist das die Schicht block5_conv3).\n",
    "- Gradienten: Die Gradienten der Zielklasse werden rückwärts durch das Netz propagiert, um die Wichtigkeit jeder Feature-Map zu berechnen.\n",
    "- Heatmap: Die Heatmap wird durch eine gewichtete Kombination der Feature-Maps erzeugt und auf das Originalbild projiziert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb57c24",
   "metadata": {},
   "source": [
    "## Die Frage der Fragen: was passiert nun, wenn wir ein Bild mit mehreren Objekten in das Modell geben :)? Probieren Sie es z.B. mal mit einem Bild von einem Hund und einer Katze aus!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
